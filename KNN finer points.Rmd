---
title: "Week 1 HW - ISYE 6501"
date: "May 23, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question 2.1

We would like to predict which secondary students will need remedial math services at the start of the school year. This is a binary classification problem, so we may try using classifiers such as logistic regression, svm, knn etc. 

Some predictors we can use are as follows (all are for the previous school year):
Math class grade, Standardized test scores, Number of days student was absent from school. The analysis may be performed at a district level in order to identify all students who need additional services.

### Question 2.2


1: Load relevant libraries and read in the data:

```{r, include=FALSE}
library(kernlab)
library(kknn)
data <- read.table("2.2credit_card_dataSummer2018.txt", stringsAsFactors = FALSE, header=FALSE)
```

Setting up the provided model:

```{r, include=T}
model <- ksvm(as.matrix(data[,1:10]),as.factor(data[,11]),type="C-svc",kernel="vanilladot",C=100,scaled=TRUE)
```

Calculate $a_1,...,a_m$:
```{r}
a<-colSums(model@xmatrix[[1]]*model@coef[[1]])
a
a0<- -model@b
a0

```

The equation of our SVM classifier is

$a_1 V_1 + a_2 V_2 + a_3 V_3 +a_4 V_4 +a_5 V_5 +a_6 V_6 +a_7 V_7 +a_8 V_8 +a_9 V_9 +a_{10} V_{10} = a_{0}$

with the coefficients as determined above. Now we see what the model predicts:

```{r}
pred <- predict(model, data[,1:10])

```

Percent of model's predictions that match the actual data:

```{r}
sum(pred==data[,11])/nrow(data) * 100
```

Question: Can we choose a value of $C$ in ksvm that yields higher accuracy? We set up some test values of $C$ to choose the correct order of magnitude, and then loop the model over each choice of $C$ to see what yields the highest accuracy. Our test vector was 

\[ C_{vals}=c(0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000)\]



```{r, echo=FALSE, results="hide"}
#Initialize vectors of C-values and accuracy placeholders:
C_vals=c(0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000)
acc=rep(0,12)

for (i in 1:12){
  #fit model using training set
  model_scaled <- ksvm(as.matrix(data[,1:10]), as.factor(data[,11]), type='C-svc', kernel='vanilladot', C=C_vals[i], scaled=TRUE)
  
  #Compare models using the validation set
  pred <- predict(model_scaled, data[,1:10])
  acc[i] <- sum(pred == data$V11)/nrow(data)
}
```

```{r, echo=FALSE}
#Find maximum accuracy and C-value associated with it
cat("Best SVM is model number ", which.max(acc[1:12]), "\n")
cat("Best C value is ", C_vals[which.max(acc[1:12])], "\n")
cat("Best validation set correctness is ", max(acc[1:12]), "\n")

#retrain the best model
model_scaled <- ksvm(as.matrix(data[,1:10]), as.factor(data[,11]), type='C-svc', kernel='vanilladot', C=C_vals[which.max(acc[1:12])], scaled=TRUE)

#test set performance
cat("Performance on test data = ", sum(predict(model_scaled, data[,1:10]) == data$V11) / nrow(data), "\n")


```

We see that $C=0.01$ yields the highest accuracy of the possible $C$ values, with a validation set accuracy of 0.8639144. Now the question becomes whether we can find a higher validation accuracy by searching more carefully close to 0.01. We run the same loop as above, just with a more precise vector of C's: $C_{vals} = seq(0.001, 0.05, by=0.00001)$. Please reference my R code to see the implementation.

```{r, echo=FALSE, results="hide"}
#Initialize vectors of C-values and accuracy placeholders:
C_vals <- seq(0.001, 0.05, by=0.00001)
acc=rep(0,length(C_vals))

for (i in 1:length(C_vals)){
  #fit model using training set
  model_scaled <- ksvm(as.matrix(data[,1:10]), as.factor(data[,11]), type='C-svc', kernel='vanilladot', C=C_vals[i], scaled=TRUE)
  
  #Compare models using the validation set
  pred <- predict(model_scaled, data[,1:10])
  acc[i] <- sum(pred == data$V11)/nrow(data)
}

#Find maximum accuracy and C-value associated with it
cat("Best SVM is model number ", which.max(acc[1:length(C_vals)]), "\n")
cat("Best C value is ", C_vals[which.max(acc[1:length(C_vals)])], "\n")
cat("Best validation set correctness is ", max(acc[1:length(C_vals)]), "\n")

#retrain the best model
model_scaled <- ksvm(as.matrix(data[,1:10]), as.factor(data[,11]), type='C-svc', kernel='vanilladot', C=C_vals[which.max(acc[1:12])], scaled=TRUE)

#test set performance
cat("Performance on test data = ", sum(predict(model_scaled, data[,1:10]) == data$V11) / nrow(data), "\n")


```

This method yields C=0.00139 with validation accuracy of 0.8685015 (improvement of 0.0045871).

2: (optional)

3: Implement knn, find a good value of k, and show how well it classifies the data in the full dataset.

We follow the code provided in TA video 3 to loop over the values of k (from 1 to 100) and choose the model with the highest accuracy:

```{r, echo=FALSE, results="hide"}
predicted<-rep(0,nrow(data))
accuracy<-rep(0,100)
for (kval in 1:100){
  for (i in 1:nrow(data)){
  model<-kknn(V11~., data[-i,],data[i,],k=kval,scale=TRUE) #set up knn model with k iterable
 predicted[i]<-as.integer(fitted(model)+0.5)  #calculate predictions for each row i of data
  }

  #Calculate fraction of correct predictions for each value of k
  accuracy[kval]<-sum(predicted==data[,11])/nrow(data)
  
  }


k_loop<-which.max(accuracy) #find optimal value of k
acc_k<-accuracy[k_loop] #find accuracy associated with this k value
```

```{r, echo=FALSE}
#print results
cat("Desired k value: ", k_loop, "\n")
cat("Associated accuracy: ", acc_k, "\n")

```



### Question 3.1

(a) We want to implement the k-nearest-neighbors classifier using cross-validation. We use the caret package's built-in cross validation feature to decide on the number of clusters, and decided on a 70-30 train-test split. The caret package produces some nice graphics:

```{r, include=FALSE}
library(caret)
```

```{r, echo=FALSE}
set.seed(200)

#Split data into train/test sets
indxTrain <- createDataPartition(y = data$V11,p = 0.70,list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]

#Run knn:
ctrl <- trainControl(method="repeatedcv",repeats = 3)
knnFit <- train(as.factor(V11) ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"),tuneLength = 20)
knnFit

#Plot results
plot(knnFit)

```

We see that the optimal number of clusters is 27. The accuracy associated with this value of $k$ is 0.8516709.





(b) We now break up the data into training/test/validation sets manually. We chose a 70/15/15 train/test/validate split.

```{r, echo=FALSE, results="hide"}
#Setting seed to ensure reproducible results
set.seed(1)

#Number of rows in dataset
n <- nrow(data)

#Split data into 70% training, 15% test, 15% validation
val<- sample(1:n, size=round(0.3*n), replace=FALSE)

d.train <- data[-val,] #Training data
d.split <- data[val,] #Test data to be split further

#Repeat same process to split data further into validation and test sets:
m<-nrow(d.split)

#Split d.split into 50% test, 50% validation
val2<- sample(1:m, size=round(0.5*m), replace=FALSE)

d.test <- d.split[-val2,] #Final test data
d.validate <- d.split[val2,] #Final validation data

```
We choose a svm classifier and iterate over the values of C (they are the same as in 2.1) to choose the best classifier on the validation set.

```{r, echo=FALSE, results="hide"}
C_vals=c(0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000)
acc=rep(0,12)

for (i in 1:12){
  #fit model using training set
  model_scaled <- ksvm(as.matrix(d.train[,1:10]), as.factor(d.train[,11]), type='C-svc', kernel='vanilladot', C=C_vals[i], scaled=TRUE)
  
  #Compare models using the validation set
  pred <- predict(model_scaled, d.validate[,1:10])
  acc[i] <- sum(pred == d.validate$V11)/nrow(d.validate)
}
cat("accuracies: ", acc, "\n")
#Find maximum accuracy and C-value associated with it
cat("Best SVM is model number ", which.max(acc[1:12]), "\n")
cat("Best C value is ", C_vals[which.max(acc[1:12])], "\n")
cat("Best validation set correctness is ", max(acc[1:12]), "\n")

#retrain the best model
model_scaled <- ksvm(as.matrix(d.train[,1:10]), as.factor(d.train[,11]), type='C-svc', kernel='vanilladot', C=C_vals[which.max(acc[1:12])], scaled=TRUE)

#test set performance
cat("Performance on test data = ", sum(predict(model_scaled, d.test[,1:10]) == d.test$V11) / nrow(d.test), "\n")


```

Our results show that the value of $C$ yielding the highest accuracy is again 0.01. As before, we investigate values close to this more carefully. We set $C_{vals} = seq(0.001, 0.05, by=0.00001)$ and iterate through to find the value of $C$ with highest validation accuracy. The final chosen value of C is 0.00196, with a re-trained test set performance of 0.8979592.

```{r, echo=FALSE, results="hide"}
#Initialize vectors of C-values and accuracy placeholders:
C_vals <- seq(0.001, 0.05, by=0.00001)
acc=rep(0,length(C_vals))

for (i in 1:length(C_vals)){
  #fit model using training set
  model_scaled <- ksvm(as.matrix(d.train[,1:10]), as.factor(d.train[,11]), type='C-svc', kernel='vanilladot', C=C_vals[i], scaled=TRUE)
  
  #Compare models using the validation set
  pred <- predict(model_scaled, d.validate[,1:10])
  acc[i] <- sum(pred == d.validate$V11)/nrow(d.validate)
}
cat("accuracies: ", acc, "\n")
#Find maximum accuracy and C-value associated with it
cat("Best SVM is model number ", which.max(acc[1:length(C_vals)]), "\n")
cat("Best C value is ", C_vals[which.max(acc[1:length(C_vals)])], "\n")
cat("Best validation set correctness is ", max(acc[1:length(C_vals)]), "\n")

#retrain the best model
model_scaled <- ksvm(as.matrix(d.train[,1:10]), as.factor(d.train[,11]), type='C-svc', kernel='vanilladot', C=C_vals[which.max(acc[1:length(C_vals)])], scaled=TRUE)

#test set performance
cat("Performance on test data = ", sum(predict(model_scaled, d.test[,1:10]) == d.test$V11) / nrow(d.test), "\n")


```
\[ \]
\[ \]
\[ \]
\[ \]




