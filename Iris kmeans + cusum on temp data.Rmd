---
title: "Week 2 HW - ISYE 6501"
date: "May 26, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(magrittr)
library(dplyr)
library(ggplot2)
library(outliers)
library(corrplot)
library(gridExtra)
```

### Question 4.2

The iris data set iris.txt contains 150 data points, each with four predictor variables and one
categorical response. The predictors are the width and length of the sepal and petal of flowers and the
response is the type of flower. The data is available from the R library datasets and can be accessed with
iris once the library is loaded. The response values are only given to see how well a
specific method performed and should not be used to build the model.

Use the R function kmeans to cluster the points as well as possible. Report the best combination of
predictors, your suggested value of k, and how well your best clustering predicts flower type.

Solution: Possible predictors are sepal.width, sepal.length, petal.width, and petal.length. The response is a categorical variable indicating the type of flower for each data point. 


```{r}
data(iris)



ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) + 
  geom_point(alpha = 0.4, size = 2) + geom_point() + 
  scale_color_manual(values = c('blue', 'red', 'green'))


#Heatmap of correlations

```

We are not going to use the Species information to help us cluster the data, but it helps us illustrate what the problems will be for the unsupervised methods. While the Setosa species is located a large distance away from either the Versicolor or Virginica data, the latter two are much closer together so there is going to be some error in the predictions. 

Data Prep: Scale the data:

```{r}
# Remove species column (5) and scale the data
iris.scaled <- scale(iris[, -5])
```


Correlation plot of the data:

```{r}
corrplot(cor(iris.scaled))

```


Graphs of the potential pairs of predictors (unscaled):

```{r}
p1 <- ggplot(iris, aes(Petal.Length, Petal.Width, color=Species)) + geom_point()
p2 <- ggplot(iris, aes(Petal.Length, Sepal.Width, color=Species)) + geom_point()
p3 <- ggplot(iris, aes(Petal.Length, Sepal.Length, color=Species)) + geom_point()
p4 <- ggplot(iris, aes(Petal.Width, Sepal.Width, color=Species)) + geom_point()
p5 <- ggplot(iris, aes(Petal.Width, Sepal.Length, color=Species)) + geom_point()
p6 <- ggplot(iris, aes(Sepal.Width, Sepal.Length, color=Species)) + geom_point()

#grid.arrange(p1,p2,p3,p4,p5,p6, cols=3)
```


Elbow method for kmeans clustering:

```{r}
# model
test_model <-  iris %>% 
    select(1:4) %>% 
    kmeans(., 3)
  
# accuracy  
tc <- test_model$cluster
species <- tc %>% unique
  
tc[which(tc == species[1])] <- "setosa"
tc[which(tc == species[2])] <- "versicolor"
tc[which(tc == species[3])] <- "virginica"
  
sum(tc == iris$Species)/nrow(iris)
```

```{r}
set.seed(123)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
data <- iris.scaled
wss<-rep(0,11)

for (i in 1:11){
wss[i] <- sapply(1:k.max, 
        function(k){kmeans(data[,Predictors[i]], k, nstart=10 )$tot.withinss})
}

#Generate plot for each entry of wss
plot(1:k.max, wss,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
abline(v = 3, lty =2)

```

### Question 5.2
Using the crime dataset, test to see whether there are any outliers in the last column (number of crimes per 100,000 people). Use the grubbs.test function in the outliers package in R.
```{r}
crime=read.csv("5.1uscrimeSummer2018.txt",stringsAsFactors = FALSE, header=TRUE, sep='\t')


```

This dataset gives state values of crime occurences for 47 states in the year 1960. We start our by plotting a boxplot of the Crime variable:






```{r}
summary(crime$Crime)

#Basic plot of the Crime column. 
boxplot(crime$Crime)

which.max(crime$Crime)

crime[which.max(crime$Crime),]

```

Investigating this data point further yields that it is a state with a population of roughly 300,000 and it's not a southern state. Consulting state population data from 1960, we see that the state is probably 


```{r}
#testing if there is one outlier in the dataset
grubbs.test(crime$Crime, type=10)
```

This yields a p-value of 0.07887 - while small, it is not below the standard threshold of significance (0.05) and thus we do not reject the null hypothesis (which is that there are no outliers). 

### Question 6.2

(1) Using July through October daily-high-temperature data for Atlanta for 1996 through 2015, use
a CUSUM approach to identify when unofficial summer ends (i.e., when the weather starts
cooling off) each year. You can use R if you'd like, but it's straightforward enough that an Excel spreadsheet can easily do the job too.

Solution: For the first part of this question, we are looking for values of $\mu$ and 

We find that the hottest month of the year for Atlanta is July, so we use the average of all July temperatures to get$\mu=88.75$. The general CUSUM formula for decrease detection is as follows:

\[ S_t = max\{0, S_{t-1} + (\mu - x_t -C)\} \]


```{r}
temps=read.csv("6.2tempsSummer2018.txt",stringsAsFactors = FALSE, header=TRUE, sep='\t')

library(qcc)
cusum(temps[,2:21])
```

Want day, temp and $S_t$ for each year for this question

(2) Use a CUSUM approach to make a judgment of whether Atlanta's summer climate has gotten
warmer in that time (and if so, when).



References:

Question 4:

(1) Heirarchical Clustering in R
https://www.r-bloggers.com/hierarchical-clustering-in-r-2/

(2) Kmeans Clustering in R
https://www.r-bloggers.com/k-means-clustering-in-r/

(3) Determining the optimal number of clusters
http://www.sthda.com/english/wiki/print.php?id=239

Question 5:

https://en.wikipedia.org/wiki/1960_United_States_Census


Question 6: